{"name":"Terracuda","tagline":"A high-level API for low-level parallelism in Lua","body":"### Summary\r\nWe will create a CUDA API for Lua aimed at programmers unfamiliar with GPU-level parallelism.\r\n\r\n### Background\r\n[Lua](http://www.lua.org/) is a fast, lightweight, and embeddable scripting language found in places like Wikipedia, World of Warcraft, Photoshop Lightroom, and more. Lua's simple syntax and dynamic typing also make it an ideal language for novice programmers. Traditionally, languages like Lua find themselves abstracted miles above low-level parallel frameworks like [CUDA](http://www.nvidia.com/object/cuda_home_new.html), and consequently GPU parallelism was limited to programmers using a systems language like C++. Frameworks like [Terra](http://terralang.org/), however, work to close that gap, making low-level programming accessible in a high-level interface. However, these interfaces still require a number of calls to C libraries and intimate knowledge of the CUDA library. For example, the following code runs a simple CUDA kernel in Terra:\r\n\r\n    terra foo(result : &float)\r\n        var t = tid()\r\n        result[t] = t\r\n    end\r\n\r\n    local R = terralib.cudacompile({ bar = foo })\r\n\r\n    terra run_cuda_code(N : int)\r\n        var data : &float\r\n        C.cudaMalloc([&&opaque](&data),sizeof(float)*N)\r\n        var launch = terralib.CUDAParams { 1,1,1, N,1,1, 0, nil }\r\n        R.bar(&launch,data)\r\n        var results : &float = [&float](C.malloc(sizeof(float)*N))\r\n        C.cudaMemcpy(results,data,sizeof(float)*N,2)\r\n        return results;\r\n    end\r\n\r\n    results = run_cuda_code(16)\r\n\r\nOther high-level CUDA bindings like [PyCUDA](http://documen.tician.de/pycuda/) and [JCuda](http://www.jcuda.org/samples/samples.html) suffer the same problem.\r\n\r\n### The Challenge\r\nThe problem is challenging foremost on the level of architecture. Designing an API is never easy, and attempting to expose GPU-level parallelism to a language as high-level as Lua requires a great deal of care to be usable while still being useful. Creating such an API requires significant knowledge of the abstraction layers between Lua, C, and CUDA as well as knowledge of the typical use cases for high-level parallelism. \r\n\r\nMy partner and I know neither Terra nor LLVM (which Terra compiles to), so creating these high-level bindings requires a great deal of initial investment. The existing interface between Terra and CUDA is sketchy at best, so we will need to implement significant new functionality into Terra in order for the Circle Renderer to function properly.\r\n\r\n### Resources\r\nFor machines, we'll just be using any computers equipped with NVIDIA GPUs (i.e. Will's laptop and the Gates 5k machines). No other special hardware/software will be needed. We'll be building upon the Terra language and also using [LuaGL](http://luagl.sourceforge.net/) for some of the demos.\r\n\r\n### Goals\r\nThe project has three main areas: writing the API, creating programs using the API, and benchmarking the code against other languages/compilers.\r\n\r\nWe plan to achieve:\r\n\r\n* **Writing the API**\r\n    * Allow arbitrary Lua code to be executed in the GPU over a table.\r\n    * Optimize threads/warp usage to the input data.\r\n    * Abstract the API such that the user needs no C libraries and as little Terra as possible.\r\n* **Creating programs**\r\n    * Make a simple saxpy\r\n    * Write matrix operations like transpose or pseudoinverse/SVD\r\n    * Port the Assignment 2 Circle Renderer over to vanilla Lua (using LuaGL)\r\n* **Benchmarking**\r\n    * For each program, benchmark it against equivalent implementations in: vanilla Lua, Terra without CUDA, and C.\r\n\r\nWe hope to achieve:\r\n\r\n* Achieve better performance than vanilla C.\r\n* Implement shared memory in Terra.\r\n* Implement linking against libraries like [cublas](https://developer.nvidia.com/cublas).\r\n\r\n### Platform\r\nCUDA makes sense as we've already learned it in class, and Lua makes sense as Terra already laid the foundation for abstracting systems-level code.\r\n\r\n### Schedule\r\n\r\n* __Friday, April 11__: finish map primitives (ie any Lua code and map over a Lua table in CUDA). Write saxpy and corresponding benchmarks.\r\n* __Friday, April 18__: complete Terracuda API. Write matrix code and benchmarks.\r\n* __Friday, April 25__: Port over Circle Renderer and benchmarks. Gather all requisite data and perform preliminary analysis.\r\n* __Friday, May 2__: Optimize/refactor API based on code written and data found. Search for possible performance gains in the abstraction layer. Attempt to implement library linking.\r\n* __Friday, May 9__: create writeup based on finalized API. Add any remaining features, time permitting (.g. shared memory).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}